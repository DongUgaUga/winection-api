import numpy as np
from dataclasses import dataclass
from collections import Counter, deque
from typing import Optional
from sklearn.metrics.pairwise import cosine_similarity
from core.log.logging import logger
from tensorflow.keras.models import load_model, Model

# âœ… ëª¨ë¸ ë° ìž„ë² ë”© ë ˆì´ì–´ ì¤€ë¹„
base_model = load_model("src/resources/sign_model2_func.h5")
dummy_input = np.zeros((1, 30, 225), dtype=np.float32)
base_model.predict(dummy_input)  # build ê°•ì œ ìˆ˜í–‰

intermediate_model = Model(
    inputs=base_model.input,
    outputs=base_model.get_layer("embedding").output
)

# âœ… í´ëž˜ìŠ¤ ì •ë³´ ë¡œë“œ
class_names = np.load("src/resources/class_names2.npy", allow_pickle=True)
class_embeddings = np.load("src/resources/class_embeddings.npy")
num_classes = len(class_names)

# âœ… ì„¤ì •ê°’
WINDOW_SIZE = 30
DISTANCE_THRESHOLD = 0.3
MISSING_HAND_FRAME_THRESHOLD = 7
PERSIST_THRESHOLD = 2
TOP_K = 5
HISTORY_SIZE = 10

# âœ… ë¬´ì˜ë¯¸í•œ ë‹¨ì–´ ë¸”ëž™ë¦¬ìŠ¤íŠ¸
BLACKLIST_WORDS = {"ì™¼ìª½-ëˆˆ", "ì™¼ìª½-ê·€"}

@dataclass
class SignPredictorState:
    label_history: deque = deque(maxlen=HISTORY_SIZE)
    missing_hand_counter: int = 0
    last_label: Optional[str] = None
    confirmed_words: list = None

    def __post_init__(self):
        self.confirmed_words = []

def normalize_landmarks(frame: list[dict]) -> np.ndarray:
    if len(frame) != 75:
        raise ValueError("landmark ê°œìˆ˜ê°€ 75ê°œê°€ ì•„ë‹ˆë‹¤.")
    coords = np.array([[lm["x"], lm["y"], lm["z"]] for lm in frame], dtype=np.float32)
    return coords.flatten()

def is_hand_missing(frame: np.ndarray) -> bool:
    left_hand = frame[33*3 : 54*3]
    right_hand = frame[54*3 : 75*3]
    return np.allclose(left_hand, 0.0) and np.allclose(right_hand, 0.0)

def sign_to_text(sequence: dict, state: SignPredictorState) -> list[str]:
    pose = sequence.get("pose", [])
    if len(pose) < WINDOW_SIZE:
        return []

    try:
        frames = [normalize_landmarks(frame) for frame in pose[-WINDOW_SIZE:]]
    except Exception as e:
        logger.warning(f"[ì •ê·œí™” ì‹¤íŒ¨] {e}")
        return []

    # âœ… ì† ì‚¬ë¼ì§ ê°ì§€
    latest_frame = frames[-1]
    if is_hand_missing(latest_frame):
        state.missing_hand_counter += 1
        # logger.info(f"ðŸ– ì† ì‚¬ë¼ì§ ì¹´ìš´í„°: {state.missing_hand_counter}")
        if state.missing_hand_counter >= MISSING_HAND_FRAME_THRESHOLD:
            result = state.confirmed_words.copy()
            if result:
                logger.info(f"ë¬¸ìž¥ ì¢…ë£Œ - ëˆ„ì  ë‹¨ì–´: {result}")
            state.confirmed_words.clear()
            state.label_history.clear()
            state.last_label = None
            state.missing_hand_counter = 0
            return result
        return []  # ì†ì´ ì•ˆ ë³´ì´ë©´ ì˜ˆì¸¡ ìžì²´ ì¤‘ë‹¨
    else:
        state.missing_hand_counter = 0

    # âœ… ì˜ˆì¸¡ ìˆ˜í–‰
    input_tensor = np.array(frames, dtype=np.float32).reshape(1, WINDOW_SIZE, -1)
    embedding = intermediate_model.predict(input_tensor, verbose=0)[0]
    similarities = cosine_similarity([embedding], class_embeddings)[0]

    top_indices = similarities.argsort()[-TOP_K:][::-1]
    top_labels = [(class_names[i], float(similarities[i])) for i in top_indices]
    top_label, top_score = top_labels[0]

    # âœ… ë¸”ëž™ë¦¬ìŠ¤íŠ¸ í•„í„°ë§
    if top_label in BLACKLIST_WORDS:
        return []

    if top_score >= 1 - DISTANCE_THRESHOLD:
        state.label_history.append(top_label)

    if len(state.label_history) >= PERSIST_THRESHOLD:
        most_common, count = Counter(state.label_history).most_common(1)[0]
        if count >= PERSIST_THRESHOLD and most_common != state.last_label:
            logger.info(f"ë‹¨ì–´: {most_common} (ìµœê·¼ {count}íšŒ ë°˜ë³µ)")
            state.confirmed_words.append(most_common)
            state.last_label = most_common
            state.label_history.clear()

    return []
